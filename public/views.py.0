import sys, datetime, csv
from django.shortcuts import render_to_response
from django.http import HttpResponse
from django.core.cache import cache
from django.views.decorators.cache import cache_page
from django.db.models import Avg, Max, Min, Count
from gass.public.models import AblationMeasurement, B1Ablation, B2Ablation
from gass.bering.loaders import Lat, Lng, Date, Time
from math import sqrt
from decimal import Decimal
from fractions import Fraction

################################################################################
# Convenience Functions ########################################################

def get_previous_record(model, record):
    '''Grab the previous record in time assuming roughly 1-hour intervals.'''
    if isinstance(record, object):
        this_datetime = record.datetime
    elif isinstance(record, dict):
        this_datetime = record['datetime']

    # Looking for a record no more than 2 hours prior
    target_datetime = this_datetime - datetime.timedelta(seconds=(60*60*2))
    new_records = model.objects.filter(
        datetime__lt=this_datetime,
        datetime__gte=target_datetime)

    return new_records[len(new_records) - 1]



def rectify(value):
    '''Fixes compulsive float values anywhere.'''
    if isinstance(value, float):
        return Decimal(str(value))
    return value



def rename_aggregate_fields(query_set):
    '''Removes the Django-appended aggregate identifiers from field names.'''
    new_query_set = []
    fields = query_set[0].keys()

    for record in query_set:
        dictionary = {}

        for field in fields:

            if field[-5:] in ['__avg', '__min', '__max']:
                dictionary[field[:-5]] = record[field]
                
        new_query_set.append(dictionary)

    return new_query_set



################################################################################
# Filtering ####################################################################

def auto_date_average(model, fields):
    '''
    Automatically calculates a simple daily average for a given data model.
    Accepts:
        model   {models.Model}  A Django model for the data
        fields  {List}          A list containing at least one field to average
    Returns:
        {QuerySet}  List, containing dictionaries with field names such as
                    fieldname__avg
    '''
    the_fields = []
    restricted_fields = ['datetime']
    for field in fields:

        if field not in restricted_fields:
            the_fields.append(Avg(field))
        else:
            the_fields.append(Min(field))

    return model.objects.exclude(hdop__gte=5).values('date').annotate(*the_fields).order_by('date')



def weighted_moving_average(window, records, shape='triangle', fields=None):
    '''
    Calculates a weighted moving average for a list of values. This function
    can also do simple (rectangular) moving averages (SMA).
    Accepts:
        window  {Integer}   Number of points to average on; length of SMA window
        records {List}      Records to perform the average on
        shape   {String}    Shape of the window; 'rectangle' or 'triangle'
        fields  {List}      Name of key(s) to grab value from (if records is a
                            list of dictionaries)
    Returns:
        {List}  List of "smoothed" values or of dictionaries with smoothed values
    '''



    def remap(hashing):
        '''Create a dictionary for each value from a series of lists.'''
        records = []
        keys = hashing.keys()
        number_of_records = len(hashing[keys[0]]) # Better be the same for all keys
        for key in keys:
            assert len(hashing[key]) == number_of_records, "Each list in the input dictionary must have the same length."

        i = 0 # Initialize counter
        while i < number_of_records:
            dictionary = {}
            for key in keys:
                dictionary[key] = hashing[key][i]
            records.append(dictionary)
            i += 1 # Increment counter

        return records
            
    

    def sequence(listing):
        '''Create a list for each key in a dictionary.'''
        dictionary = {}

        for key in listing[0].keys():
            dictionary[key] = [] # Initalize data_dict with the field names

        for each in listing:
            # For each dictionary...

            for item in each.items():
                # For each (key, value) pair...
                dictionary[item[0]].append(item[1]) # Sequence the values

        return dictionary



    def wma(values, weights):
        '''Peforms weighted average over a list of values with the given weights.'''
        averaged_values = []

        i = 0 # Initialize counter for values
        while i < len(values):

            start = i-(window/2) # Rounds down always
            end = i+(window/2) + 1 # Grab edge of next bin (requires +1)

            if start < 0 or end >= len(values):
                # Exclude edge effects by truncating the series
                averaged_values.append(None)

            else:
                holder = [] # Temporary holder for convolution

                j = 0 # Initialize counter for subset
                subset = values[start:i] # Grab the left-hand side (backwards glance)
                while j < len(weights):
                    holder.append(weights[j]*subset[j])
                    j += 1 # Increment counter for subset

                weights.reverse() # IMPORTANT: Reverse the weights for symmetry

                k = 0 # Initialize counter for subset
                subset = values[i+1:end] # Grab the right-hand side (forwards glance)
                while k < len(weights):
                    holder.append(weights[k]*subset[k])
                    k += 1 # Increment counter for subset

                holder.append(values[i])
                averaged_values.append(sum(holder)/3)
                weights.reverse() # IMPORTANT: Reverse back to normal for next loop

            i += 1 # Increment counter for values

        return averaged_values



    def truncate(values):
        '''
        Truncates a list of values according to how they would be truncated
        under the WMA; useful for fields not to be averaged.
        '''
        trunc_values = []

        i = 0 # Initialize counter for values
        while i < len(values):

            start = i-(window/2) # Rounds down always
            end = i+(window/2) + 1 # Grab edge of next bin (requires +1)

            if start < 0 or end >= len(values):
                # Exclude edge effects by truncating the series
                trunc_values.append(None)

            else:
                trunc_values.append(values[i])

            i += 1 # Increment counter for values

        return trunc_values



    if shape == 'triangle':

        if window % 2 == 0:
            window = window - 1 # Make sure window is an odd number (required)

        averaged_records = []
        ramps = [] # Scales for the "sides" of the triangular window

        for num in range(1, (window/2) + 1):
            ramps.append(Decimal(str(float(Fraction(num, (window/2) + 1)))))

        assert sum(ramps) == Decimal('1.0'), "Non-unity for sum of weights in triangular window."

        data_dict = {}

        if fields:
            raw_data_dict = sequence(records)

            for field in raw_data_dict.keys():

                # If the field is one specified in the function call...
                if field in fields:
                    data_dict[field] = wma(raw_data_dict[field], ramps)
                else:
                    data_dict[field] = truncate(raw_data_dict[field])

            return remap(data_dict) # Create dictionary for each record

        else:
            return wma(records, ramps)

    else:
        return simple_moving_average(window, records)



def simple_moving_average(window, records):
    '''
    Calculates a simple moving average (SMA) for a list of values.
    Accepts:
        window  {Integer}   Number of points to average on; length of SMA window
        records {List}      The values to perform an SMA on
    Returns:
        {List}  List of "smoothed" values from the SMA
    '''
    averaged_records = []

    if window % 2 != 0:
        window + 1 # Make sure window is an even number (for easy SMA)

    i = 0 # Initialize counter
    while i < len(records):
        start = i-(window/2)
        if start < 0:
            start = 0 # Python wraps negative indices; we don't want that
        subset = records[start:(i+(window/2))] # Grab a window's length of data
        averaged_records.append(sum(subset)/len(subset))
        i += 1 # Incerement counter

    return averaged_records



################################################################################
# Calculations #################################################################

def melt_degree_days(model, fields=None):
    '''
    Calculates and returns the MDDs for a date series based on a base date.
    Accepts additional fields to be returned in a list as the fields parameter.
    Accepts:
        model   {models.Model}  A Django model for the data
        fields  {List}          Sequence of field names to also be averaged
    Returns:
        {List}  Sequence of dictionaries with the following fields:
            date        Julian date or other ordinal for the date
            mdds        The MDDs for that day
            accum_mdds  The cumulative MDDs for that day
        All other fields passed to the function are formatted: fieldname__avg
    '''
    cq = cache.get('melt_degree_days' + str(model)[-12:-2]) # Retrieve the cache
    if cq is not None:
        # If a cached copy exists, return it
        return cq

    # First day that the minimum temperature exceeded 28 degrees F was March 28
    #   but this is actually the Julian day reset
    first_julian_day = datetime.date(2011, 1, 1).toordinal()
    base_temp = Decimal('0.0') # Freezing in Celsius

    annotation = []
    if fields:
        for field in fields:
            annotation.append(Avg(field))
    annotation.extend([Min('temp_C'), Max('temp_C')])

    date_series = model.objects.values('date').annotate(*annotation).order_by('date')

    cumulative_mdds = 0
    mdd_series = []
    for record in date_series:

        max_temp = record['temp_C__max']
        min_temp = record['temp_C__min']

        if max_temp < base_temp:
            mdds = 0
        elif (max_temp + min_temp)/2 < base_temp:
            mdds = (max_temp - base_temp)/4
        elif min_temp <= base_temp:
            mdds = (max_temp - base_temp)/2 - (base_temp - min_temp)/4
        elif min_temp > base_temp:
            mdds = (max_temp + min_temp)/2 - base_temp
        else:
            return ValueError("Unhandled exception in Melt Degree Days calculation.")

        cumulative_mdds += mdds # Accumulate MDDs

        data_dict = {
            'date': record['date'].toordinal() - first_julian_day,
            'mdds': mdds,
            'accum_mdds': cumulative_mdds
            }

        if fields:
            for field in fields:
                data_dict[field] = record[field + '__avg']

        mdd_series.append(data_dict)

    # Cache result for 1 hour (60 times 60 seconds)
    cache.set('melt_degree_days' + str(model)[-12:-2], mdd_series, 60*60)

    return mdd_series



def geographic_distance(this, last):
    '''
    Calculates the distance, in meters, between two geographic coordinates.
    Accepts:
        this    {QuerySet}  QuerySet of one record representing the current observation
        last    {QuerySet}  QuerySet of one record representing the last observation
    Returns:
        {Float} The net migration, in meters, between the two observations
    '''
    lat_m_per_degree = Decimal('111412.0')
    lng_m_per_degree = Decimal('55800.0')
    # 111,412 m/degree of latitude at 60 degrees north latitude
    #   (from National Geospatial Intelligence Agency)
    # 55,800 m/degree of longitude at 60 degrees north latitude
    #   (from National Geospatial Intelligence Agency)
    # http://msi.nga.mil/MSISiteContent/StaticFiles/Calculators/degree.html


    if isinstance(this, dict) and isinstance(last, dict):
        if not this['lat'] or not this['lng'] or not last['lat'] or not last['lng']:
            return None
        this_lat = rectify(this['lat'])
        this_lng = rectify(this['lng'])
        last_lat = rectify(last['lat'])
        last_lng = rectify(last['lng'])

    else:
        if not this[0].lat or not this[0].lng or not last[0].lat or not last[0].lng:
            return None
        this_lat = rectify(this[0].lat)
        this_lng = rectify(this[0].lng)
        last_lat = rectify(last[0].lat)
        last_lng = rectify(last[0].lng)

    lat_diff_m = abs(lat_m_per_degree*(this_lat - last_lat))
    lng_diff_m = abs(lng_m_per_degree*(this_lng - last_lng))

    # Simple distance estimate in meters between last and last observation
    distance_m = sqrt((lat_diff_m*lat_diff_m) + (lng_diff_m*lng_diff_m))

    return distance_m



def displacement_series(positions):
    '''
    Calculates displacements in time and space in an ordered series of
    geographic coordinates.
    Accepts:
        positions   {List}  List of dictionaries with (at least) the following keys:
            datetime    {datetime.datetime}
            lat         {Decimal}
            lng         {Decimal}
    '''


    def calc_displacement(displacement, time_offset):
        '''
        Essentially a quality check for displacements; assumption is that
        ablation sites can't move more than 5 meters in 
        '''
        # Reasonable movement less than 2 meters per hour
        upper_limit = 2*((time_offset.seconds/60.0)/60.0)
        if 0 <= displacement < upper_limit:
            return displacement
        else:
            return 0.0



    displacements = [{
        'displacement': 0,
        'datetime_diff': datetime.timedelta(0,0,0),
        'datetime': positions[0]['datetime']
        }]
    number_of_positions = len(positions)
    
    i = 1 # Initialize counter
    while i < number_of_positions:

        if positions[i]['datetime'] and positions[i-1]['datetime']:
            displacement = geographic_distance(positions[i], positions[i-1])
            datetime_diff = positions[i]['datetime'] - positions[i-1]['datetime']
            displacements.append({
                'displacement': displacement,
                'datetime_diff': datetime_diff,
                'datetime': positions[i]['datetime']
                })

        i += 1 # Increment counter

    return displacements



def migration_series(displacements):
    '''
    Calculates migration over an ordered series of displacements in time.
    Accepts:
        positions   {List}  List of dictionaries with (at least) the following keys:
            datetime        {datetime.datetime}
            datetime_diff   {datetime.timedelta}
            displacement    {Float}
    Returns:
        {List}  Sequence of migration and migration rate calculations
            migration_m
            migration_rate_m_per_day
            datetime
    '''
    migrations = []
    last_distance = 0

    for record in displacements:
        # Calculate the time offset in days (for a daily rate)
        datetime_offset = record['datetime_diff'].days
        datetime_offset += record['datetime_diff'].seconds/(60.0*60.0*24.0)
        # Calculate the total distance travelled thus far based on previous total
        total_distance = record['displacement'] + last_distance

        if datetime_offset:
            # Guard against zero division
            migrations.append({
                'migration_m': total_distance,
                'migration_rate_m_per_day': record['displacement']/datetime_offset,
                'datetime': record['datetime']
                })
        last_distance = total_distance

    return migrations



def summary_migration_and_ablation(model, latest, first):
    '''
    Calculates the near-real time current conditions in migrations and ablation.
    Accepts:
        model   {models.Model}  The Django data model of interest
        latest  {Object}        The latest record available
        first   {Object}        The first record available
    Returns:
        {Dictionary}    The near-real time migration and ablation calculations
            migration_m
            migration_rate_m_per_day
            ablation_cm
            ablation_rate_cm_per_day
    '''
    average_window = 6 # Length of window to average by (hours, roughly)
    if average_window % 2 == 0:
        window = average_window - 1
    cutoff = window/2 # For handling edge effects



    def migration(model):
        cq = cache.get('summary_migration' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq

        fields = ('datetime', 'lat', 'lng')
        averaged_values = auto_date_average(model, fields)
        values = rename_aggregate_fields(averaged_values)
        values.append({
            'datetime': latest.datetime,
            'lat': latest.lat,
            'lng': latest.lng
            }) # Add in the most recent position

        # Migration calculation
        migrations = migration_series(displacement_series(values))
        migration_m = migrations[len(migrations)-1]['migration_m'] 
        # IMPORTANT: Get second-latest migration rate because the most recent record was added
        migration_rate_m_per_day = migrations[len(migrations)-2]['migration_rate_m_per_day']

        result = {
            'migration_m': migration_m,
            'migration_rate_m_per_day': migration_rate_m_per_day
            }

        # Cache result for 1 hour (60 times 60 seconds)
        cache.set('summary_migration' + str(model)[-12:-2], result, 60*60)

        return result



    def ablation(model):
        cq = cache.get('summary_ablation' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq



        def ablation_rate(hours):
            this = values[len(values) - 1]
            previous = values[len(values) - hours] # Assumption is measurements are taken every hour
            time_delta = this['datetime'] - previous['datetime'] # Guard against consequences of assumption
            hours_offset = time_delta.days*Decimal('24.0') + (time_delta.seconds/(Decimal('60.0')*Decimal('60.0')))
            ablation_diff = this['acoustic_range_cm'] - previous['acoustic_range_cm']

            if ablation_diff:
                # Get the ablation rate for 12 hours, doubled for an estimated daily rate
                multiplier = Decimal('24.0')/hours_offset # We want a daily rate, regardless of time offset
                ablation_rate_cm_per_day = ablation_diff*multiplier

                if ablation_rate_cm_per_day > 20 or ablation_rate_cm_per_day < 2:
                    ablation_rate_cm_per_day = ablation_rate(hours*2)

            elif first.acoustic_range_cm and latest.acoustic_range_cm:
                # If there's a problem with the daily calculation above, do it the old way
                time_offset = latest.datetime - first.datetime

                if time_offset:
                    ablation_rate_cm_per_day = (latest.acoustic_range_cm - first.acoustic_range_cm)/time_offset.days

                else:
                    ablation_rate_cm_per_day = 0

            else:
                ablation_rate_cm_per_day = 0

            return ablation_rate_cm_per_day



        fields = ('datetime', 'acoustic_range_cm')
        raw_values = model.objects.values('datetime', 'acoustic_range_cm').order_by('datetime')
        average_values = weighted_moving_average(average_window, raw_values, 'triangle', ['acoustic_range_cm'])
        values = average_values[cutoff:len(average_values)-(cutoff+1)] # Remove edge effects

        # Use raw values for simple ablation calculation
        ablation_cm = values[len(values)-1]['acoustic_range_cm']
        ablation_cm -= first.acoustic_range_cm # Remove baseline

        ablation_rate_cm_per_day = ablation_rate(24)

        result = {
            'ablation_cm': ablation_cm,
            'ablation_rate_cm_per_day': ablation_rate_cm_per_day
            }

        # Cache result for 1 hour (60 times 60 seconds)
        cache.set('summary_ablation' + str(model)[-12:-2], result, 60*60)

        return result



    migration = migration(model)
    ablation = ablation(model)

    return {
        'migration_m':              migration['migration_m'],
        'migration_rate_m_per_day': migration['migration_rate_m_per_day'],
        'ablation_cm':              ablation['ablation_cm'],
        'ablation_rate_cm_per_day': ablation['ablation_rate_cm_per_day']
        }



################################################################################
# Display and Rendering ########################################################

def display_index(request):

    return render_to_response('index.html')



def display_data_access(request):

    return render_to_response('data.html')



def display_about(request):
    '''Display the "About the Project" web page.'''

    return render_to_response('about.html', {})



def display_instrumentation(request):

    return render_to_response('instruments.html')



def display_test(request):

    return render_to_response('test.html')



def display_sidebar(request):
    '''Display the current status sidebar on the Home page.'''

    data_dictionary = {}

    for dataset in [['b1', B1Ablation], ['b2', B2Ablation]]:
        data = dataset[0]
        model = dataset[1]

        # Find the latest measurement
        late_date = model.objects.all().aggregate(Max('datetime'))
        latest = model.objects.exclude(hdop__gte=5).filter(datetime__exact=late_date['datetime__max'])[0]

        # Find the first measurement
        first_date = model.objects.all().aggregate(Min('datetime'))
        first = model.objects.exclude(hdop__gte=5).filter(datetime__exact=first_date['datetime__min'])[0]

        # Get a tuple of migration, ablation, and their rates
        metrics = summary_migration_and_ablation(model, latest, first)

        data_dictionary[data] = {
            'first': first,
            'latest': latest,
            'migration_m': metrics['migration_m'],
            'migration_rate': metrics['migration_rate_m_per_day'],
            'ablation_cm': metrics['ablation_cm'],
            'ablation_rate': metrics['ablation_rate_cm_per_day'],
            }

    return render_to_response('sidebar.html', data_dictionary)



def display_current(request):
    '''Display HTML page of website index.'''


    def calc_migration_track_date_average(model):
        '''Calculates a date average of the migration track (geographic coordinates).'''
        cq = cache.get('migration_track_date_avg' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq

        averaged_coords = auto_date_average(model, ['lat', 'lng'])
       
        # Cache result for 1 hour (60 times 60 seconds)
        cache.set('migration_track_date_avg' + str(model)[-12:-2], averaged_coords, 60*60)

        return averaged_coords



    def calc_albedo(latest):
        '''Calculates the latest albedo.'''
        cq = cache.get('latest_albedo' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq

        # Calculate the albedo from either an object or dictionary
        try:
            top_light = float(latest.top_light)
            bottom_light = float(latest.bottom_light)
        except:
            top_light = float(latest['top_light'])
            bottom_light = float(latest['bottom_light'])

        if top_light != 0 and bottom_light <= top_light:
            latest_albedo = bottom_light/top_light # Check for dividing by zero

        elif top_light != 0 and bottom_light > top_light:
            latest_albedo = 1.0
            # calc_albedo(get_previous_record(model, latest)) # Check for greater reflectance than irradiance

        else:
            latest_albedo = 0.0

        # Cache result for 1 hour (60 times 60 seconds)
        cache.set('latest_albedo' + str(model)[-12:-2], latest_albedo, 60*60) 

        return latest_albedo



    data_dictionary = {}
    for dataset in [['b1', B1Ablation], ['b2', B2Ablation]]:
        data = dataset[0]
        model = dataset[1]

        # Find the latest measurement
        late_date = model.objects.all().aggregate(Max('datetime'))
        latest = model.objects.exclude(hdop__gte=5).filter(datetime__exact=late_date['datetime__max'])[0]

        # Find the first measurement
        first_date = model.objects.all().aggregate(Min('datetime'))
        first = model.objects.exclude(hdop__gte=5).filter(datetime__exact=first_date['datetime__min'])[0]

        # Get a tuple of migration, ablation, and their rates
        metrics = summary_migration_and_ablation(model, latest, first)
        latest_albedo = calc_albedo(latest)
        # Series of coordinate pairs representing migration
        track = calc_migration_track_date_average(model)

        data_dictionary[data] = {
            'first': first,
            'latest': latest,
            'track': track,
            'migration_m': metrics['migration_m'],
            'migration_rate': metrics['migration_rate_m_per_day'],
            'ablation_cm': metrics['ablation_cm'],
            'ablation_rate': metrics['ablation_rate_cm_per_day'],
            'latest_albedo': latest_albedo
            }

    return render_to_response('current.html', data_dictionary)



def display_plot_conditions(request, site=None):
    '''Display the time series plots for a given GASS site.'''



    def get_temperatures(model):
        cq = cache.get('temperature_plot' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq
        else:
            values = model.objects.values('datetime', 'temp_C').order_by('datetime')
            query = []
            for record in values:
                if record['temp_C'] is None:
                    query.append((record['datetime'], 'null'))
                else:
                    query.append((record['datetime'], record['temp_C']))
            # Cache result for 1 hour (60 times 60 seconds)
            cache.set('temperature_plot' + str(model)[-12:-2], query, 60*60) 
            return query



    templates = {
        None:   'plot_conditions.html',
        'b01':  'b1_plot.html',
        'b02':  'b2_plot.html'
        }
    template = templates[site]

    data_dictionary = {}

    for dataset in [['b1', B1Ablation], ['b2', B2Ablation]]:
        data = dataset[0]
        model = dataset[1]
        temperature_obs = get_temperatures(model)
        daily_temperature_obs = auto_date_average(model, ['temp_C'])
        data_dictionary[data] = {
            'temperature': temperature_obs,
            'daily_temperature': daily_temperature_obs
            }
    
    return render_to_response(template, data_dictionary)



def display_plot_mdds(request):
    '''Display the "Real-Time Plots" web page for melt degree day metrics.'''
    average_window = 6 # Length of window to average by
    if average_window % 2 == 0:
        average_window = average_window - 1



    def calc_melt_degree_days(model):
        '''Calculate the number of Melt Degree Days (MDDs) given the appropriate data model.'''
        # Get daily and cumulative MDDs along with acoustic range
        mdd_series = melt_degree_days(model, ['acoustic_range_cm'])
        baseline = mdd_series[0]['acoustic_range_cm']

        for record in mdd_series:
            record['ablation'] = record['acoustic_range_cm'] - baseline
            days_of_observation = record['date'] - mdd_series[0]['date']
            if days_of_observation:
                record['ablation_rate'] = record['ablation']/days_of_observation
            else:
                record['ablation_rate'] = Decimal('0.0')
            record.pop('acoustic_range_cm') # Remove the old key

        return mdd_series



    data_dictionary = {}

    for dataset in [['b1', B1Ablation], ['b2', B2Ablation]]:
        data = dataset[0]
        model = dataset[1]
        mdd_obs = calc_melt_degree_days(model)
        min_mdds = 100000 # Will be interpolated
        max_mdds = 0 # Will be interpolated
        for item in mdd_obs:
            if item['mdds'] > max_mdds:
                max_mdds = item['mdds']
            elif item['mdds'] < min_mdds:
                min_mdds = item['mdds']
        data_dictionary[data] = {
            'degree_days': mdd_obs,
            'max_mdds': max_mdds,
            'min_mdds': min_mdds,
            'max_cumulative_mdds': mdd_obs[len(mdd_obs) - 1]['accum_mdds'],
            }

    return render_to_response('plot_mdds.html', data_dictionary)



def display_plot_ablation(request):
    '''Display the "Real-Time Plots" web page for ablation metrics.'''
    average_window = 6 # Length of window to average by
    if average_window % 2 == 0:
        average_window = average_window - 1


    def calc_ablation(model):
        '''Calculate daily ablation given the appropriate data model.'''
        cq = cache.get('ablation_plot' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq

        else:
            values = model.objects.values('datetime', 'acoustic_range_cm').order_by('datetime')
            average_values = weighted_moving_average(average_window, values, 'triangle', ['acoustic_range_cm'])
            first = average_values[average_window/2] # Average truncates the time series
            baseline = first['acoustic_range_cm'] # Remove the baseline (about 100 cm)
            query = []

            for record in average_values:

                if record['acoustic_range_cm']:
                    query.append((record['datetime'], record['acoustic_range_cm'] - baseline))

            # Cache result for 1 hour (60 times 60 seconds)
            cache.set('ablation_plot' + str(model)[-12:-2], query, 60*60) 

            return query



    def calc_ablation_rate(ablation_obs):
        '''Calculate the ablation rate given the appropriate data model.'''
        cq = cache.get('ablation_rate_plot' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq

        else:
            ablation_rates = []
            first_obs = ablation_obs[0] # First ablation observation

            for obs in ablation_obs:
                days = obs[0] - first_obs[0] # Number of days

                if obs[0] and days.days:
                    # Non-zero value check
                    ablation_rates.append((obs[0], obs[1]/days.days))

            # Cache result for 1 hour (60 times 60 seconds)
            cache.set('ablation_rate_plot' + str(model)[-12:-2], ablation_rates, 60*60) 

            return ablation_rates



    data_dictionary = {}

    for dataset in [['b1', B1Ablation], ['b2', B2Ablation]]:
        data = dataset[0]
        model = dataset[1]
        ablation_obs = calc_ablation(model)
        data_dictionary[data] = {
            'ablation': ablation_obs
            }

    return render_to_response('plot_ablation.html', data_dictionary)



def display_plot_migration(request):
    '''Display the "Real-Time Plots" web page.'''
    average_window = 12 # Length of window to average by
    if average_window % 2 == 0:
        window = average_window - 1
    cutoff = window/2 # For handling edge effects



    def calc_migration_and_rate(model):
        cq = cache.get('migration_and_rate_plot' + str(model)[-12:-2]) # Retrieve the cache
        if cq is not None:
            # If a cached copy exists, return it
            return cq

        else:
            # Find the latest measurement
            late_date = model.objects.all().aggregate(Max('datetime'))
            latest = model.objects.exclude(hdop__gte=5).filter(datetime__exact=late_date['datetime__max'])[0]

            # Find the first measurement
            first_date = model.objects.all().aggregate(Min('datetime'))
            first = model.objects.exclude(hdop__gte=5).filter(datetime__exact=first_date['datetime__min'])[0]

            averaged_values = auto_date_average(model, fields)
            values = rename_aggregate_fields(averaged_values)
            values.append({
                'datetime': latest.datetime,
                'lat': latest.lat,
                'lng': latest.lng
                }) # Add in the most recent position

            result = migration_series(displacement_series(values))
            cache.set('migration_and_rate_plot' + str(model)[-12:-2], result, 60*60)

            return result



    data_dictionary = {}

    for dataset in [['b1', B1Ablation], ['b2', B2Ablation]]:
        data = dataset[0]
        model = dataset[1]
        fields = ('datetime', 'lat', 'lng')

        migration_obs = calc_migration_and_rate(model)
        data_dictionary[data] = {
            'migration': migration_obs[len(migration_obs) - 10:],
            'min_migration': migration_obs[len(migration_obs) - 10]['migration_m'],
            'max_migration': migration_obs[len(migration_obs) - 1]['migration_m'],
            'min_migration_rate': migration_obs[len(migration_obs) - 10]['migration_rate_m_per_day'],
            'min_date': migration_obs[0]['datetime']
            }

    return render_to_response('plot_migration.html', data_dictionary)



def display_surge(request):
    '''Display the "About Surging Glaciers" web page.'''

    return render_to_response('surge.html', {})



def display_team(request):
    '''Display the "Project Team Members" web page.'''

    return render_to_response('team.html', {})



@cache_page(60 * 60) # Cache page for 1 hours
def export_all_records(request, site):
    '''
    Export all records (entire history) for a given data source in CSV format;
    this view generates a CSV file.

    Keyword arguments:
    source  -- The data source all records are requested from;
                acceptable values are 'b01' and 'b02'
    start   -- The starting date of the time series to export;
                dates should be in the format 'YYYY-MM-DD' with leading zeroes
    end     -- The ending date of the time series to export;
                dates should be in the format 'YYYY-MM-DD' with leading zeroes
    '''
    average_window = 6 # Length of window to average by
    if average_window % 2 == 0:
        average_window = average_window - 1

    # TODO: If it is determined greater efficiency is needed, consider
    #   modifying this view so that it generates a CSV file on the server;
    #   a cronjob could call this function at a regular interval, and
    #   Apache could serve the "cached" file when requested

    models = {'b01': B1Ablation, 'b02': B2Ablation}
    model = models[site]
    response = HttpResponse(mimetype='text/csv')
    history = []
    response['Content-Disposition'] = 'attachment; filename=' + site + '_all_records.csv'

    # These are the "pretty names" of the fields written to first row
    fields = ('Satellites', 'HDOP', 'Time(UTC)', 'Date(UTC)', 'DateTime(UTC)',
        'Latitude(N)', 'Longitude(W)', 'AcousticRange(cm)', 'OpticalRange(cm)',
        'Irradiance', 'Reflectance', 'WindSpeed(m/s)', 'Temperature (C)',
        'Voltage(V)')

    for record in model.objects.all().order_by('datetime'):
        sequence = [] # Create a blank sequence to append to
        sequence.append(record.satellites)
        sequence.append(record.hdop)
        sequence.append(record.time)
        sequence.append(record.date)
        sequence.append(record.datetime)
        sequence.append(record.lat)
        sequence.append(record.lng)
        sequence.append(record.acoustic_range_cm)
        sequence.append(record.optical_range_cm)
        sequence.append(record.top_light)
        sequence.append(record.bottom_light)
        sequence.append(record.wind_m_s)
        sequence.append(record.temp_C)
        sequence.append(record.voltage)
        history.append(sequence) # Append all fields to the history

    csv_writer = csv.writer(response, dialect='excel')
    csv_writer.writerow(fields)
    csv_writer.writerows(history)
    
    return response
